{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpZlWXwEo6m4laIRRct+SK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COSMIC5545/outfit-recommender/blob/main/OpenAIGym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is designed to be run in a Google Colab notebook.\n",
        "# Please run these installation commands in a Colab cell first.\n",
        "\n",
        "# !pip install gymnasium\n",
        "# !pip install \"gymnasium[classic_control,box2d]\"\n",
        "# !pip install pygame\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "print(\"Gymnasium, NumPy, and Random imported successfully.\")\n",
        "\n",
        "# ---\n",
        "# 2. Create an environment using OpenAI Gym (now Gymnasium)\n",
        "# ---\n",
        "# We'll use \"FrozenLake-v1\". It's a simple 4x4 grid.\n",
        "# 'S' = Start, 'F' = Frozen, 'H' = Hole, 'G' = Goal\n",
        "# The render_mode=\"ansi\" gives us a text-based view.\n",
        "#\n",
        "# is_slippery=False makes the environment deterministic (action 'Right'\n",
        "# always moves Right). This is MUCH easier for a basic Q-agent to learn.\n",
        "# The default (True) is stochastic (action 'Right' might move Up or Down).\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "print(\"Environment 'FrozenLake-v1' created.\")\n",
        "\n",
        "# ---\n",
        "# 3. Define the states, actions, and rewards\n",
        "# ---\n",
        "# The environment defines these for us.\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "print(f\"Number of States: {num_states}\")   # 16 (for the 4x4 grid)\n",
        "print(f\"Number of Actions: {num_actions}\")  # 4 (Left, Down, Right, Up)\n",
        "\n",
        "# ---\n",
        "# 4. Create a reinforcement learning agent using Q-learning\n",
        "# ---\n",
        "\n",
        "# Initialize the Q-table with all zeros\n",
        "# Shape: (number of states, number of actions)\n",
        "q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "print(\"Q-Table initialized with shape:\", q_table.shape)\n",
        "\n",
        "# ---\n",
        "# 5. Train the agent using the environment\n",
        "# ---\n",
        "\n",
        "print(\"Starting agent training...\")\n",
        "\n",
        "# Hyperparameters\n",
        "total_episodes = 10000        # Total episodes to train\n",
        "learning_rate = 0.1           # Alpha: How much we update Q-values\n",
        "gamma = 0.99                  # Discount factor: Importance of future rewards\n",
        "epsilon = 1.0                 # Exploration rate: Start by 100% exploring\n",
        "max_epsilon = 1.0             # Maximum exploration rate\n",
        "min_epsilon = 0.01            # Minimum exploration rate\n",
        "epsilon_decay_rate = 0.0005   # How fast epsilon decreases\n",
        "\n",
        "# List to store rewards (for monitoring)\n",
        "training_rewards = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment for a new episode\n",
        "    # .reset() returns a tuple (initial_state, info), we just need the state\n",
        "    state = env.reset()[0]\n",
        "\n",
        "    terminated = False  # True when agent falls in a hole or reaches goal\n",
        "    truncated = False   # True when episode ends due to time limit (not in this env)\n",
        "\n",
        "    current_reward = 0\n",
        "\n",
        "    while not terminated and not truncated:\n",
        "        # --- Epsilon-greedy strategy ---\n",
        "        # Decide whether to explore or exploit\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Explore: Choose a random action\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Exploit: Choose the best action from Q-table\n",
        "            action = np.argmax(q_table[state, :])\n",
        "\n",
        "        # --- Take the action ---\n",
        "        # .step() returns (new_state, reward, terminated, truncated, info)\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # --- Q-learning update rule ---\n",
        "        # Q(s,a) = Q(s,a) + lr * [R(s,a) + gamma * max(Q(s',a')) - Q(s,a)]\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "                                 learning_rate * (reward + gamma * np.max(q_table[new_state, :]))\n",
        "\n",
        "        # Update our current state\n",
        "        state = new_state\n",
        "        current_reward += reward\n",
        "\n",
        "    # --- End of episode ---\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate * episode)\n",
        "\n",
        "    training_rewards.append(current_reward)\n",
        "\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        print(f\"Training... Episode: {episode + 1}/{total_episodes}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# ---\n",
        "# 6. Test the agent on the environment\n",
        "# ---\n",
        "\n",
        "print(\"\\nTesting the trained agent...\")\n",
        "test_episodes = 100\n",
        "total_wins = 0\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()[0]\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # print(f\"\\n--- Test Episode {episode + 1} ---\")\n",
        "\n",
        "    while not terminated and not truncated:\n",
        "        # In test mode, we ONLY exploit (no exploration)\n",
        "        action = np.argmax(q_table[state, :])\n",
        "\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "        if terminated and reward == 1.0:\n",
        "            total_wins += 1\n",
        "            # print(\"Agent reached the goal! ðŸ†\")\n",
        "        # elif terminated:\n",
        "            # print(\"Agent fell in a hole. ðŸ•³ï¸\")\n",
        "\n",
        "# ---\n",
        "# Result\n",
        "# ---\n",
        "print(\"\\n--- Test Results ---\")\n",
        "success_rate = (total_wins / test_episodes) * 100\n",
        "print(f\"Success Rate: {success_rate:.2f}%\")\n",
        "print(f\"Total wins over {test_episodes} episodes: {total_wins}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Q-Table ---\")\n",
        "# Print the Q-table values, rounded for readability\n",
        "print(np.round(q_table, 3))\n",
        "\n",
        "# ---\n",
        "# Bonus: Watch the agent play one episode\n",
        "# ---\n",
        "print(\"\\n--- Watching the agent play (text) ---\")\n",
        "state = env.reset()[0]\n",
        "terminated = False\n",
        "truncated = False\n",
        "print(env.render()) # Show initial state\n",
        "time.sleep(1)\n",
        "\n",
        "while not terminated and not truncated:\n",
        "    action = np.argmax(q_table[state, :])\n",
        "    new_state, reward, terminated, truncated, info = env.step(action)\n",
        "    state = new_state\n",
        "\n",
        "    print(f\"\\nAction: {['Left', 'Down', 'Right', 'Up'][action]}\")\n",
        "    print(env.render())\n",
        "    time.sleep(0.5)\n",
        "\n",
        "if reward == 1.0:\n",
        "    print(\"Agent reached the goal! ðŸ†\")\n",
        "else:\n",
        "    print(\"Agent fell in a hole. ðŸ•³ï¸\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jokeCSeycNmw",
        "outputId": "630fcc53-91f6-47b6-d173-8fc977b69c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gymnasium, NumPy, and Random imported successfully.\n",
            "Environment 'FrozenLake-v1' created.\n",
            "Number of States: 16\n",
            "Number of Actions: 4\n",
            "Q-Table initialized with shape: (16, 4)\n",
            "Starting agent training...\n",
            "Training... Episode: 1000/10000\n",
            "Training... Episode: 2000/10000\n",
            "Training... Episode: 3000/10000\n",
            "Training... Episode: 4000/10000\n",
            "Training... Episode: 5000/10000\n",
            "Training... Episode: 6000/10000\n",
            "Training... Episode: 7000/10000\n",
            "Training... Episode: 8000/10000\n",
            "Training... Episode: 9000/10000\n",
            "Training... Episode: 10000/10000\n",
            "Training finished.\n",
            "\n",
            "Testing the trained agent...\n",
            "\n",
            "--- Test Results ---\n",
            "Success Rate: 100.00%\n",
            "Total wins over 100 episodes: 100\n",
            "\n",
            "--- Final Q-Table ---\n",
            "[[0.941 0.951 0.951 0.941]\n",
            " [0.941 0.    0.961 0.948]\n",
            " [0.942 0.97  0.845 0.936]\n",
            " [0.937 0.    0.098 0.043]\n",
            " [0.951 0.961 0.    0.941]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.    0.98  0.    0.956]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.961 0.    0.97  0.951]\n",
            " [0.961 0.98  0.98  0.   ]\n",
            " [0.97  0.99  0.    0.97 ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.   ]\n",
            " [0.    0.977 0.99  0.967]\n",
            " [0.98  0.99  1.    0.98 ]\n",
            " [0.    0.    0.    0.   ]]\n",
            "\n",
            "--- Watching the agent play (text) ---\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "\n",
            "Action: Down\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "\n",
            "Action: Down\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "\n",
            "Action: Right\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "\n",
            "Action: Right\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "\n",
            "\n",
            "Action: Down\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "\n",
            "Action: Right\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "Agent reached the goal! ðŸ†\n"
          ]
        }
      ]
    }
  ]
}